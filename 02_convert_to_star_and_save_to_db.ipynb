{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './work/'\n",
    "parquetdir = './parquet/'\n",
    "source_csv = 'votacao_candidato_munzona_2022_BRASIL.csv'\n",
    "extracted_file = workdir +'extracted/'+source_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/07 11:30:14 WARN Utils: Your hostname, suetham-workplace resolves to a loopback address: 127.0.1.1; using 192.168.1.172 instead (on interface wlo1)\n",
      "24/02/07 11:30:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/07 11:30:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/07 11:30:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark_session = SparkSession.builder.appName('spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/suetham/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/suetham/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/suetham/.asdf/installs/python/3.10.8/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame\n\u001b[0;32m----> 3\u001b[0m df: DataFrame \u001b[39m=\u001b[39m spark_session\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m, delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m;\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mISO-8859-1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcsv(extracted_file)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.10.8/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "df: DataFrame = spark_session.read.options(header=\"true\", delimiter=\";\", encoding=\"ISO-8859-1\").csv(extracted_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining relevant columns\n",
    "relevant_columns=[\n",
    "    \"NR_TURNO\",\n",
    "    \"DS_ELEICAO\",\n",
    "    \"TP_ABRANGENCIA\",\n",
    "    \"SG_UF\",\n",
    "    \"NM_MUNICIPIO\",\n",
    "    \"NR_ZONA\",\n",
    "    \"DS_CARGO\",\n",
    "    \"NR_CANDIDATO\",\n",
    "    \"NM_CANDIDATO\",\n",
    "    \"NM_URNA_CANDIDATO\",\n",
    "    \"DS_SITUACAO_CANDIDATURA\",\n",
    "    \"NR_PARTIDO\",\n",
    "    \"SG_PARTIDO\",\n",
    "    \"NM_PARTIDO\",\n",
    "    \"NM_COLIGACAO\",\n",
    "    \"DS_COMPOSICAO_COLIGACAO\",\n",
    "    \"ST_VOTO_EM_TRANSITO\",\n",
    "    \"QT_VOTOS_NOMINAIS\",\n",
    "    \"NM_TIPO_DESTINACAO_VOTOS\",\n",
    "    \"QT_VOTOS_NOMINAIS_VALIDOS\",\n",
    "    \"DS_SIT_TOT_TURNO\"\n",
    "]\n",
    "\n",
    "# Selecting relevant columns\n",
    "selected_columns_df = df.select(relevant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing first lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.dataframe import DataFrame, Column\n",
    "\n",
    "def get_columns_list_from_dimension(dimension: Dict[str, List[str]]):\n",
    "    return [col for cols in dimension for col in cols]\n",
    "\n",
    "def get_table_name_and_records(dataframe: DataFrame, dimension_table_name_and_columns: Dict[str, List[str]]) -> List[tuple[str, DataFrame]]:\n",
    "    dimensions = []\n",
    "\n",
    "    for dimension_table_name, dimension_columns in dimension_table_name_and_columns:\n",
    "        dimension_records = dataframe.select(*dimension_columns).distinct()\n",
    "        surrogate_key_column_name = f\"sk_{dimension_table_name.replace('dim_', '')}\"\n",
    "\n",
    "        # add unique and increasing id to dimension (but not consecutive)\n",
    "        unique_and_increasing_id = F.monotonically_increasing_id()\n",
    "        dimension_records = dimension_records.withColumn(\n",
    "            surrogate_key_column_name,\n",
    "            unique_and_increasing_id\n",
    "        )\n",
    "\n",
    "        dimension_table_in_tuple = (dimension_table_name, dimension_records)\n",
    "\n",
    "        dimensions.append(dimension_table_in_tuple)\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "\n",
    "def transform_spark_dataframe_into_star_schema(\n",
    "    original_dataframe: DataFrame,\n",
    "    fact_columns: List[str]  = [\"col1\", \"col2\"],\n",
    "    fact_table_name = \"tabela_fato\",\n",
    "    mapping_dimension_columns: Dict[str, List[str]] = {'dim1':[\"col3\", \"col4\"], \"dim2\":[\"col5\", \"col6\"]},\n",
    "):\n",
    "    dimension_columns_separated_by_dimension = mapping_dimension_columns.values()\n",
    "\n",
    "    dimension_columns = get_columns_list_from_dimension(dimension_columns_separated_by_dimension)\n",
    "\n",
    "    columns_from_fact_and_dimension = fact_columns + dimension_columns\n",
    "\n",
    "    original_dataframe = original_dataframe.select(*columns_from_fact_and_dimension)\n",
    "\n",
    "    dimension_table_name_and_columns = mapping_dimension_columns.items()\n",
    "\n",
    "    dimensions = get_table_name_and_records(original_dataframe, dimension_table_name_and_columns)\n",
    "\n",
    "    # Substitui as colunas de dimensão pelo respectivo SK na tabela fato\n",
    "    # ------------------------------------------------------------------\n",
    "    for table_name, records in dimensions:\n",
    "        # join the dimension dataframe to the original dataframe\n",
    "        dimension_columns_by_dimension_from_dataframe = [\n",
    "            original_dataframe[column] == records[column]\n",
    "            for column in mapping_dimension_columns[table_name]\n",
    "        ]\n",
    "        # surrogate_key_column = [col for col in records.columns if \"sk\" in col][0]\n",
    "        \n",
    "        original_dataframe = original_dataframe.join(\n",
    "            F.broadcast(records), \n",
    "            on=dimension_columns_by_dimension_from_dataframe,\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # drop the original columns\n",
    "        original_dataframe = original_dataframe.drop(*mapping_dimension_columns[table_name])\n",
    "\n",
    "    fact_table = (fact_table_name, original_dataframe)\n",
    "    \n",
    "    return dimensions + [fact_table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_schema = transform_spark_dataframe_into_star_schema(\n",
    "    selected_columns_df,\n",
    "    fact_columns=[\"QT_VOTOS_NOMINAIS_VALIDOS\", \"QT_VOTOS_NOMINAIS\"],\n",
    "    fact_table_name=\"tabela_fato\",\n",
    "    mapping_dimension_columns={\n",
    "        'dim_municipio': [\"SG_UF\", \"NM_MUNICIPIO\"],\n",
    "        'dim_cargo': [\"DS_CARGO\"],\n",
    "        'dim_ds_eleicao':[\"DS_ELEICAO\"],\n",
    "        'dim_partido':[\"SG_PARTIDO\",\"NM_PARTIDO\", \"NR_PARTIDO\"],\n",
    "        'dim_candidato':[\"NM_CANDIDATO\", \"NR_CANDIDATO\", \"NM_URNA_CANDIDATO\"],\n",
    "        'dim_turno':[\"NR_TURNO\"],\n",
    "        'dim_tp_agrangencia':[\"TP_ABRANGENCIA\"],\n",
    "        'dim_zona':[\"NR_ZONA\"],\n",
    "        'dim_situacao_candidatura':[\"DS_SITUACAO_CANDIDATURA\"],\n",
    "        'dim_coligacao':[\"NM_COLIGACAO\", \"DS_COMPOSICAO_COLIGACAO\"],\n",
    "        \"dim_voto_transito\":[\"ST_VOTO_EM_TRANSITO\"],\n",
    "        'dim_situacaof_turno':[\"DS_SIT_TOT_TURNO\"],\n",
    "        'dim_destinacao_voto':[\"NM_TIPO_DESTINACAO_VOTOS\"]\n",
    "    },   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname_or_ip = \"0.0.0.0\"\n",
    "port = \"55555\"\n",
    "db = \"star\"\n",
    "user = \"star\"\n",
    "password = \"star\"\n",
    "\n",
    "db_url = \"jdbc:postgresql://\" + hostname_or_ip + \":\" + port + \"/\" + db\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in star_schema:\n",
    "    table_name, original_dataframe = item\n",
    "    print(f\"Writing {table_name} to DW\")\n",
    "    original_dataframe.write.jdbc(url=db_url, table=table_name, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark session\n",
    "# spark_session.stop()\n",
    "\n",
    "# Cleaning up files \n",
    "# Delete the directory and all its contents\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(workdir+'extracted/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
