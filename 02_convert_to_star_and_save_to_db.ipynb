{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = './work/'\n",
    "parquetdir = './parquet/'\n",
    "source_csv = 'votacao_candidato_munzona_2022_BRASIL.csv'\n",
    "extracted_file = workdir +'extracted/'+source_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark_session = SparkSession.builder.appName('spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.options(header=\"true\", delimiter=\";\", encoding=\"ISO-8859-1\").csv(extracted_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining relevant columns\n",
    "relevant_columns=[\n",
    "    \"NR_TURNO\",\n",
    "    \"DS_ELEICAO\",\n",
    "    \"TP_ABRANGENCIA\",\n",
    "    \"SG_UF\",\n",
    "    \"NM_MUNICIPIO\",\n",
    "    \"NR_ZONA\",\n",
    "    \"DS_CARGO\",\n",
    "    \"NR_CANDIDATO\",\n",
    "    \"NM_CANDIDATO\",\n",
    "    \"NM_URNA_CANDIDATO\",\n",
    "    \"DS_SITUACAO_CANDIDATURA\",\n",
    "    \"NR_PARTIDO\",\n",
    "    \"SG_PARTIDO\",\n",
    "    \"NM_PARTIDO\",\n",
    "    \"NM_COLIGACAO\",\n",
    "    \"DS_COMPOSICAO_COLIGACAO\",\n",
    "    \"ST_VOTO_EM_TRANSITO\",\n",
    "    \"QT_VOTOS_NOMINAIS\",\n",
    "    \"NM_TIPO_DESTINACAO_VOTOS\",\n",
    "    \"QT_VOTOS_NOMINAIS_VALIDOS\",\n",
    "    \"DS_SIT_TOT_TURNO\"\n",
    "]\n",
    "\n",
    "# Selecting relevant columns\n",
    "selected_columns_df = df.select(relevant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing first lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def transform_spark_dataframe_into_star_schema(\n",
    "    dataframe,\n",
    "    colunas_fato = [\"col1\", \"col2\"],\n",
    "    tabela_fato_nome = \"tabela_fato\",\n",
    "    mapping_colunas_dimensao = {'dim1':[\"col3\", \"col4\"], \"dim2\":[\"col5\", \"col6\"]},\n",
    "):\n",
    "\n",
    "    colunas_fato_e_dimensao = colunas_fato + [col for cols in mapping_colunas_dimensao.values() for col in cols]\n",
    "    dataframe = dataframe.select(*colunas_fato_e_dimensao)\n",
    "\n",
    "    dimensions = []\n",
    "    for dim, cols in mapping_colunas_dimensao.items():\n",
    "\n",
    "        df_dimension = dataframe.select(*cols).distinct()\n",
    "        sk_name = f\"sk_{dim.replace('DIM_', '')}\"\n",
    "        # add unique id to dimension\n",
    "        df_dimension = df_dimension.withColumn(sk_name, F.monotonically_increasing_id())\n",
    "\n",
    "        dimensions.append( (dim, df_dimension) )\n",
    "\n",
    "\n",
    "\n",
    "    # Substitui as colunas de dimens√£o pelo respectivo SK na tabela fato\n",
    "    # ------------------------------------------------------------------\n",
    "    for dim, df_dimension in dimensions:\n",
    "        # join the dimension dataframe to the original dataframe\n",
    "        dataframe = dataframe.join(\n",
    "            df_dimension, \n",
    "            on=[\n",
    "                dataframe[col] == df_dimension[col]\n",
    "                for col in mapping_colunas_dimensao[dim]\n",
    "            ],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # drop the original columns\n",
    "        dataframe = dataframe.drop(*mapping_colunas_dimensao[dim])\n",
    "    return dimensions + [ (tabela_fato_nome, dataframe) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_schema = transform_spark_dataframe_into_star_schema(\n",
    "    selected_columns_df,\n",
    "    colunas_fato=[\"QT_VOTOS_NOMINAIS_VALIDOS\", \"QT_VOTOS_NOMINAIS\"],\n",
    "    tabela_fato_nome=\"tabela_fato\",\n",
    "    mapping_colunas_dimensao={\n",
    "        'dim_municipio': [\"SG_UF\", \"NM_MUNICIPIO\"],\n",
    "        'dim_cargo': [\"DS_CARGO\"],\n",
    "        'dim_ds_eleicao':[\"DS_ELEICAO\"],\n",
    "        'dim_partido':[\"SG_PARTIDO\",\"NM_PARTIDO\", \"NR_PARTIDO\"],\n",
    "        'dim_candidato':[\"NM_CANDIDATO\", \"NR_CANDIDATO\", \"NM_URNA_CANDIDATO\"],\n",
    "        'dim_turno':[\"NR_TURNO\"],\n",
    "        'dim_tp_agrangencia':[\"TP_ABRANGENCIA\"],\n",
    "        'dim_zona':[\"NR_ZONA\"],\n",
    "        'dim_situacao_candidatura':[\"DS_SITUACAO_CANDIDATURA\"],\n",
    "        'dim_coligacao':[\"NM_COLIGACAO\", \"DS_COMPOSICAO_COLIGACAO\"],\n",
    "        \"dim_voto_transito\":[\"ST_VOTO_EM_TRANSITO\"],\n",
    "        'dim_situacaof_turno':[\"DS_SIT_TOT_TURNO\"],\n",
    "        'dim_destinacao_voto':[\"NM_TIPO_DESTINACAO_VOTOS\"]\n",
    "    },   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname_or_ip = \"dw\"\n",
    "port = \"5432\"\n",
    "db = \"star\"\n",
    "user = \"star\"\n",
    "password = \"star\"\n",
    "schema = \"star\"\n",
    "\n",
    "\n",
    "db_url = \"jdbc:postgresql://\" + hostname_or_ip + \":\" + port + \"/\" + db\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in star_schema:\n",
    "    table_name,dataframe = item\n",
    "    print(f\"Writing {table_name} to DW\")\n",
    "    dataframe.write.jdbc(url=db_url, table=schema+\".\"+table_name, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark session\n",
    "spark_session.stop()\n",
    "\n",
    "# Cleaning up files \n",
    "# Delete the directory and all its contents\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(workdir+'extracted/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
